apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: loadpp-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.19, pipelines.kubeflow.org/pipeline_compilation_time: '2023-07-18T14:07:20.665169',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "load pp pipeline", "inputs":
      [{"name": "dataset_str", "type": "String"}, {"name": "train_test_split", "type":
      "Boolean"}], "name": "loadpp"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.19}
spec:
  entrypoint: loadpp
  templates:
  - name: load-tensorflow-dataset-component
    container:
      args: [--dataset-str, '{{inputs.parameters.dataset_str}}', --train-test-split,
        '{{inputs.parameters.train_test_split}}', --data-file-output, /tmp/outputs/data_file_output/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'tensorflow' 'tensorflow_datasets' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'tensorflow' 'tensorflow_datasets'
        --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef load_tensorflow_dataset_component(dataset_str, \n                  \
        \                    train_test_split, \n                                \
        \      data_file_output):\n\n        import tensorflow_datasets as tfds\n\
        \        import json\n\n        # define function\n        def load_tensorflow_dataset(dataset_str,\
        \ \n                                    train_test_split = True):\n      \
        \          # assign train_test_split param\n                if train_test_split\
        \ is True:\n                        split = ['train', 'test']\n          \
        \      else:\n                        split = 'all'\n\n                # load\n\
        \                data, ds_info = tfds.load(\n                        dataset_str,\n\
        \                        split=split, shuffle_files=True,\n              \
        \          as_supervised=True,\n                        with_info=True,\n\
        \                )\n\n                # package data\n                if train_test_split\
        \ is True:\n                        data = {'train': data[0], \n         \
        \                       'test': data[1]}\n                else:\n        \
        \                data = {'train': data,\n                                'test':\
        \ None}\n                return data\n\n        # output is a dictionary with\
        \ 'test' and 'train' keys. \n        data = load_tensorflow_dataset(dataset_str=dataset_str,\
        \ \n                                        train_test_split=train_test_split)\n\
        \n        # save output data \n        data['train'].save(data_file_output)\n\
        \        # with open(data_file_output, \"wb\") as file:\n        #       \
        \  file.write(data['train'].to_file())\n\ndef _deserialize_bool(s) -> bool:\n\
        \    from distutils.util import strtobool\n    return strtobool(s) == 1\n\n\
        import argparse\n_parser = argparse.ArgumentParser(prog='Load tensorflow dataset\
        \ component', description='')\n_parser.add_argument(\"--dataset-str\", dest=\"\
        dataset_str\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --train-test-split\", dest=\"train_test_split\", type=_deserialize_bool, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--data-file-output\"\
        , dest=\"data_file_output\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = load_tensorflow_dataset_component(**_parsed_args)\n"
      image: python:3.9
    inputs:
      parameters:
      - {name: dataset_str}
      - {name: train_test_split}
    outputs:
      artifacts:
      - {name: load-tensorflow-dataset-component-data_file_output, path: /tmp/outputs/data_file_output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--dataset-str", {"inputValue": "dataset_str"}, "--train-test-split",
          {"inputValue": "train_test_split"}, "--data-file-output", {"outputPath":
          "data_file_output"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''tensorflow''
          ''tensorflow_datasets'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''tensorflow'' ''tensorflow_datasets''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef load_tensorflow_dataset_component(dataset_str,
          \n                                      train_test_split, \n                                      data_file_output):\n\n        import
          tensorflow_datasets as tfds\n        import json\n\n        # define function\n        def
          load_tensorflow_dataset(dataset_str, \n                                    train_test_split
          = True):\n                # assign train_test_split param\n                if
          train_test_split is True:\n                        split = [''train'', ''test'']\n                else:\n                        split
          = ''all''\n\n                # load\n                data, ds_info = tfds.load(\n                        dataset_str,\n                        split=split,
          shuffle_files=True,\n                        as_supervised=True,\n                        with_info=True,\n                )\n\n                #
          package data\n                if train_test_split is True:\n                        data
          = {''train'': data[0], \n                                ''test'': data[1]}\n                else:\n                        data
          = {''train'': data,\n                                ''test'': None}\n                return
          data\n\n        # output is a dictionary with ''test'' and ''train'' keys.
          \n        data = load_tensorflow_dataset(dataset_str=dataset_str, \n                                        train_test_split=train_test_split)\n\n        #
          save output data \n        data[''train''].save(data_file_output)\n        #
          with open(data_file_output, \"wb\") as file:\n        #         file.write(data[''train''].to_file())\n\ndef
          _deserialize_bool(s) -> bool:\n    from distutils.util import strtobool\n    return
          strtobool(s) == 1\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Load
          tensorflow dataset component'', description='''')\n_parser.add_argument(\"--dataset-str\",
          dest=\"dataset_str\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-test-split\",
          dest=\"train_test_split\", type=_deserialize_bool, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--data-file-output\",
          dest=\"data_file_output\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = load_tensorflow_dataset_component(**_parsed_args)\n"], "image": "python:3.9"}},
          "inputs": [{"name": "dataset_str", "type": "String"}, {"name": "train_test_split",
          "type": "Boolean"}], "name": "Load tensorflow dataset component", "outputs":
          [{"name": "data_file_output", "type": "tf.data.Dataset"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"dataset_str": "{{inputs.parameters.dataset_str}}",
          "train_test_split": "{{inputs.parameters.train_test_split}}"}'}
  - name: loadpp
    inputs:
      parameters:
      - {name: dataset_str}
      - {name: train_test_split}
    dag:
      tasks:
      - name: load-tensorflow-dataset-component
        template: load-tensorflow-dataset-component
        arguments:
          parameters:
          - {name: dataset_str, value: '{{inputs.parameters.dataset_str}}'}
          - {name: train_test_split, value: '{{inputs.parameters.train_test_split}}'}
      - name: preprocess-component
        template: preprocess-component
        dependencies: [load-tensorflow-dataset-component]
        arguments:
          artifacts:
          - {name: load-tensorflow-dataset-component-data_file_output, from: '{{tasks.load-tensorflow-dataset-component.outputs.artifacts.load-tensorflow-dataset-component-data_file_output}}'}
  - name: preprocess-component
    container:
      args: [--data-file-input, /tmp/inputs/data_file_input/data, --pp-data-file-output,
        /tmp/outputs/pp_data_file_output/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'numpy' 'tensorflow' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'numpy' 'tensorflow' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef preprocess_component(data_file_input, \n                         pp_data_file_output):\n\
        \    import json\n    import numpy as np\n    import tensorflow as tf\n\n\
        \    # define preprocess function\n    def preprocess_mnist_tfds(image, label=None):\n\
        \        # reshape and upsample to 3 channel for transfer learning models\n\
        \        # ... for when no channel information is present\n        if len(image.shape)\
        \ != 3:\n            image = np.dstack((image, image, image))\n        # ...\
        \ for when channel is only 1 dimension\n        if image.shape[2] == 1:\n\
        \            image = tf.image.grayscale_to_rgb(image)\n        # normalize\
        \ pixel values\n        image = tf.cast(image, tf.float32) / 255.\n      \
        \  # resize with pad for mobilenetv2\n        image = tf.image.resize_with_pad(image,\
        \ target_height=224, target_width=224)\n        return image, label\n\n  \
        \  # load data from previous step\n    # with open(data_file_input, 'rb')\
        \ as f:\n    data = tf.data.Dataset.load(data_file_input)\n\n    data = data.map(preprocess_mnist_tfds,\
        \ num_parallel_calls=tf.data.AUTOTUNE)\n    data = data.batch(128)\n\n   \
        \ # # preprocess and batch \n    # for key, value in data.items():\n    #\
        \     if value is not None:\n    #         data[key] = value.map(preprocess_mnist_tfds,\
        \ num_parallel_calls=tf.data.AUTOTUNE)\n    #         data[key] = data[key].batch(128)\n\
        \n    # save output data \n    # with open(pp_data_file_output, \"wb\") as\
        \ file:\n    #         file.write(data.to_file())\n    data.save(pp_data_file_output)\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Preprocess component',\
        \ description='')\n_parser.add_argument(\"--data-file-input\", dest=\"data_file_input\"\
        , type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --pp-data-file-output\", dest=\"pp_data_file_output\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = preprocess_component(**_parsed_args)\n"
      image: python:3.9
    inputs:
      artifacts:
      - {name: load-tensorflow-dataset-component-data_file_output, path: /tmp/inputs/data_file_input/data}
    outputs:
      artifacts:
      - {name: preprocess-component-pp_data_file_output, path: /tmp/outputs/pp_data_file_output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--data-file-input", {"inputPath": "data_file_input"}, "--pp-data-file-output",
          {"outputPath": "pp_data_file_output"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''numpy'' ''tensorflow''
          || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''numpy'' ''tensorflow'' --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef preprocess_component(data_file_input,
          \n                         pp_data_file_output):\n    import json\n    import
          numpy as np\n    import tensorflow as tf\n\n    # define preprocess function\n    def
          preprocess_mnist_tfds(image, label=None):\n        # reshape and upsample
          to 3 channel for transfer learning models\n        # ... for when no channel
          information is present\n        if len(image.shape) != 3:\n            image
          = np.dstack((image, image, image))\n        # ... for when channel is only
          1 dimension\n        if image.shape[2] == 1:\n            image = tf.image.grayscale_to_rgb(image)\n        #
          normalize pixel values\n        image = tf.cast(image, tf.float32) / 255.\n        #
          resize with pad for mobilenetv2\n        image = tf.image.resize_with_pad(image,
          target_height=224, target_width=224)\n        return image, label\n\n    #
          load data from previous step\n    # with open(data_file_input, ''rb'') as
          f:\n    data = tf.data.Dataset.load(data_file_input)\n\n    data = data.map(preprocess_mnist_tfds,
          num_parallel_calls=tf.data.AUTOTUNE)\n    data = data.batch(128)\n\n    #
          # preprocess and batch \n    # for key, value in data.items():\n    #     if
          value is not None:\n    #         data[key] = value.map(preprocess_mnist_tfds,
          num_parallel_calls=tf.data.AUTOTUNE)\n    #         data[key] = data[key].batch(128)\n\n    #
          save output data \n    # with open(pp_data_file_output, \"wb\") as file:\n    #         file.write(data.to_file())\n    data.save(pp_data_file_output)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Preprocess component'',
          description='''')\n_parser.add_argument(\"--data-file-input\", dest=\"data_file_input\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--pp-data-file-output\",
          dest=\"pp_data_file_output\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = preprocess_component(**_parsed_args)\n"], "image": "python:3.9"}}, "inputs":
          [{"name": "data_file_input", "type": "tf.data.Dataset"}], "name": "Preprocess
          component", "outputs": [{"name": "pp_data_file_output", "type": "tf.data.Dataset"}]}',
        pipelines.kubeflow.org/component_ref: '{}'}
  arguments:
    parameters:
    - {name: dataset_str}
    - {name: train_test_split}
  serviceAccountName: pipeline-runner
