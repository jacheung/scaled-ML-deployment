{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac9474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kfp.components.types.type_annotations import OutputPath, InputPath\n",
    "# from kfp.components.component_factory import create_component_from_func\n",
    "# import kfp\n",
    "\n",
    "from kfp.components import InputPath, OutputPath, create_component_from_func\n",
    "\n",
    "def load_tensorflow_dataset_component(dataset_str: str, \n",
    "                                      train_test_split: bool, \n",
    "                                      data_file_output: OutputPath('tf.data.Dataset')):\n",
    "    \n",
    "        import tensorflow_datasets as tfds\n",
    "        import json\n",
    "\n",
    "        # define function\n",
    "        def load_tensorflow_dataset(dataset_str: str, \n",
    "                                    train_test_split: bool = True):\n",
    "                # assign train_test_split param\n",
    "                if train_test_split is True:\n",
    "                        split = ['train[:20%]', 'test']\n",
    "                else:\n",
    "                        split = 'all'\n",
    "                \n",
    "                # load\n",
    "                data, ds_info = tfds.load(\n",
    "                        dataset_str,\n",
    "                        split=split, shuffle_files=True,\n",
    "                        as_supervised=True,\n",
    "                        with_info=True,\n",
    "                )\n",
    "\n",
    "                # package data\n",
    "                if train_test_split is True:\n",
    "                        data = {'train': data[0], \n",
    "                                'test': data[1]}\n",
    "                else:\n",
    "                        data = {'train': data,\n",
    "                                'test': None}\n",
    "                return data\n",
    "\n",
    "        # output is a dictionary with 'test' and 'train' keys. \n",
    "        data = load_tensorflow_dataset(dataset_str=dataset_str, \n",
    "                                        train_test_split=train_test_split)\n",
    "        \n",
    "        # save output data \n",
    "        data['train'].save(data_file_output)\n",
    "\n",
    "\n",
    "load_op = create_component_from_func(\n",
    "        func=load_tensorflow_dataset_component,\n",
    "        base_image='python:3.9',\n",
    "        packages_to_install=['tensorflow', 'tensorflow_datasets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_component(data_file_input: InputPath('tf.data.Dataset'), \n",
    "                         pp_data_file_output: OutputPath('tf.data.Dataset')):\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # define preprocess function\n",
    "    def preprocess_mnist_tfds(image, label=None):\n",
    "        # reshape and upsample to 3 channel for transfer learning models\n",
    "        # ... for when no channel information is present\n",
    "        if len(image.shape) != 3:\n",
    "            image = np.dstack((image, image, image))\n",
    "        # ... for when channel is only 1 dimension\n",
    "        if image.shape[2] == 1:\n",
    "            image = tf.image.grayscale_to_rgb(image)\n",
    "        # normalize pixel values\n",
    "        image = tf.cast(image, tf.float32) / 255.\n",
    "        # resize with pad for mobilenetv2\n",
    "        image = tf.image.resize_with_pad(image, target_height=224, target_width=224)\n",
    "        return image, label\n",
    "\n",
    "    \n",
    "    # load data from previous step\n",
    "    data = tf.data.Dataset.load(data_file_input)\n",
    "\n",
    "    data = data.map(preprocess_mnist_tfds, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    data = data.batch(128)\n",
    "\n",
    "    # # preprocess and batch \n",
    "    # for key, value in data.items():\n",
    "    #     if value is not None:\n",
    "    #         data[key] = value.map(preprocess_mnist_tfds, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    #         data[key] = data[key].batch(128)\n",
    "        \n",
    "    # save output data \n",
    "    # with open(pp_data_file_output, \"wb\") as file:\n",
    "    #         file.write(data.to_file())\n",
    "    data.save(pp_data_file_output)\n",
    "\n",
    "\n",
    "preprocess_op = create_component_from_func(\n",
    "        func=preprocess_component,\n",
    "        base_image='python:3.9',\n",
    "        packages_to_install=['numpy', 'tensorflow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5260042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def production_train_component(pp_data_file_input: InputPath('tf.data.Dataset'),\n",
    "                               experiment_name: str):\n",
    "    \n",
    "    import tensorflow_hub as hub\n",
    "    import mlflow\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import time\n",
    "\n",
    "    # mlflow Tracking requires definition of experiment name AND logged params\n",
    "    # Experiment names they should be defined as \"project-task-version\"\n",
    "    def set_mlflow_experiment(experiment_name:str, artifact_location: str = None):\n",
    "        try:\n",
    "            experiment_id = mlflow.create_experiment(experiment_name, \n",
    "                                                    artifact_location=artifact_location)\n",
    "        # except mlflow.exceptions.MlflowException as e:\n",
    "        #   if str(e) == f\"Experiment '{experiment_name}' already exists.\":\n",
    "        except:\n",
    "            print(f'Experiment already exists, setting experiment to {experiment_name}')\n",
    "            experiment_info = mlflow.set_experiment(experiment_name)\n",
    "            experiment_id = experiment_info.experiment_id\n",
    "        experiment = mlflow.get_experiment(experiment_id)\n",
    "        print(\"---------------------\")\n",
    "        print('Experiment details are:')\n",
    "        print(\"Name: {}\".format(experiment.name))\n",
    "        print(\"Experiment_id: {}\".format(experiment.experiment_id))\n",
    "        print(\"Artifact Location: {}\".format(experiment.artifact_location))\n",
    "        print(\"Creation timestamp: {}\".format(experiment.creation_time))\n",
    "        return experiment_id\n",
    "\n",
    "    class MNIST(mlflow.pyfunc.PythonModel): \n",
    "        def __init__(self, mlflow_registered_model_name: str = None):\n",
    "            self._model = None\n",
    "            self._mlflow_registered_model_name = mlflow_registered_model_name\n",
    "            self.load()    \n",
    "        @staticmethod\n",
    "        def _build(self, hyperparameters):\n",
    "            ## Build model\n",
    "            # class names for mnist hardcoded\n",
    "            class_names = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        \n",
    "            # set layer regularization for DNN\n",
    "            regularizer = tf.keras.regularizers.l1_l2(hyperparameters['l1'], hyperparameters['l2'])\n",
    "\n",
    "            # load in mobilenetv2 weights and instantiate dense classification head \n",
    "            base_model = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "            layers = [\n",
    "                hub.KerasLayer(\n",
    "                    base_model,\n",
    "                    input_shape=(224, 224, 3),\n",
    "                    trainable=False,\n",
    "                    name='mobilenet_embedding'),\n",
    "                tf.keras.layers.Dense(hyperparameters['num_hidden'],\n",
    "                                    kernel_regularizer=regularizer,\n",
    "                                    activation='relu',\n",
    "                                    name='dense_hidden'),\n",
    "                tf.keras.layers.Dense(len(class_names),\n",
    "                                    kernel_regularizer=regularizer,\n",
    "                                    activation='softmax',\n",
    "                                    name='mnist_prob')\n",
    "            ]\n",
    "\n",
    "            self._model = tf.keras.Sequential(layers, name='mnist-classification')\n",
    "\n",
    "            # compile model \n",
    "            self._model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hyperparameters['learning_rate']),\n",
    "                                loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                                from_logits=False),\n",
    "                                metrics=['accuracy'])\n",
    "            \n",
    "            # base model logging\n",
    "            self._model_base = base_model\n",
    "\n",
    "        def fit_hp_search(self, xy_train, xy_test, hyperparameters):                      \n",
    "            self._build(self, hyperparameters)\n",
    "            # fit model using train/test split to find hyperparams\n",
    "            self._train_history = self._model.fit(xy_train,\n",
    "                                                epochs=hyperparameters['epochs'],\n",
    "                                                validation_data=xy_test)\n",
    "        \n",
    "        def fit_production(self, xy_train, hyperparameters):                      \n",
    "            self._build(self, hyperparameters)\n",
    "            # fit model using all the data \n",
    "            self._train_history = self._model.fit(xy_train,\n",
    "                                                epochs=hyperparameters['epochs'])\n",
    "    \n",
    "    # instantiate model and load data\n",
    "    mnist_model = MNIST()\n",
    "    ds_train = tf.data.Dataset.load(pp_data_file_input)\n",
    "    hyperparameters = {\n",
    "    'learning_rate': 0.01,\n",
    "    'l1': 0.0,\n",
    "    'l2': 0.0, \n",
    "    'num_hidden': 16,\n",
    "    'epochs': 10}\n",
    "\n",
    "    # train model and log via mlflow\n",
    "    experiment_id = set_mlflow_experiment(experiment_name=experiment_name)\n",
    "    mlflow_run_name=f'production-{time.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "    with mlflow.start_run(experiment_id=experiment_id,  \n",
    "                            run_name=mlflow_run_name) as run:\n",
    "        mnist_model.fit_production(xy_train=ds_train,\n",
    "                                    hyperparameters=hyperparameters)\n",
    "        # MLFlow Tracking parameters\n",
    "        mlflow.log_params(params=hyperparameters)\n",
    "\n",
    "        # MLFlow Tracking metrics \n",
    "        # Logging metrics for each epoch (housed in dictionary)\n",
    "        training_history = mnist_model._train_history.history\n",
    "        for epoch in range(0, hyperparameters['epochs']):\n",
    "            insert = {}\n",
    "            for metric, value in training_history.items():\n",
    "                insert[metric] = training_history[metric][epoch]\n",
    "            mlflow.log_metrics(metrics=insert, step=epoch+1)\n",
    "\n",
    "        # MLFlow tracking artifact (e.g. model file)\n",
    "        # this will log the model and all its details under run_id/artifacts\n",
    "        # ths will also register the model so it can be served\n",
    "        result = mlflow.tensorflow.log_model(python_model=mnist_model,\n",
    "                                artifact_path=\"\",\n",
    "                                registered_model_name=experiment_name)\n",
    "\n",
    "        uri = f'{run.info.artifact_uri}/{result.artifact_path}'\n",
    "        \n",
    "        # Close out MLFlow run to prevent any log contamination.\n",
    "        mlflow.end_run(status='FINISHED') \n",
    "\n",
    "    return uri\n",
    "\n",
    "\n",
    "\n",
    "production_train_op = create_component_from_func(\n",
    "                    func=production_train_component,\n",
    "                    base_image='python:3.9',\n",
    "                    packages_to_install=['numpy', 'tensorflow', 'tensorflow_hub', 'mlflow'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cfa313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "from kubernetes.client.models import V1EnvVar\n",
    "\n",
    "@dsl.pipeline(name=\"production-pipeline\",\n",
    "              description=\"production training pipeline\")\n",
    "def load_pp_pipeline(dataset_str: str,\n",
    "                     train_test_split: bool,\n",
    "                     experiment_name: str):\n",
    "    load_task = load_op(dataset_str=dataset_str,\n",
    "                        train_test_split=train_test_split)\n",
    "    preprocess_task = preprocess_op(data_file_input=load_task.outputs['data_file_output'])\n",
    "    production_train_task = (production_train_op(pp_data_file_input=preprocess_task.outputs['pp_data_file_output'],\n",
    "                                                experiment_name=experiment_name)).add_env_variable(V1EnvVar(name='MLFLOW_TRACKING_URI',\n",
    "                                             value='http://mlflow.mlflow.svc.cluster.local'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1809bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "kfp.compiler.Compiler().compile(load_pp_pipeline, \"./pipelines/production-pipeline.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"mnist-classification\"\n",
    "mlflow_run_name = \"test_run\"\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.01,\n",
    "    'l1': 0.0,\n",
    "    'l2': 0.0, \n",
    "    'num_hidden': 16,\n",
    "    'epochs': 10}\n",
    "\n",
    "def hyperparameter_search(pp_data_file_input: InputPath('pickle'),\n",
    "                          num_runs,\n",
    "                          num_parallel):\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "    import mlflow\n",
    "    from components.utils import set_mlflow_experiment\n",
    "    from components.model_step import model\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # load data from pipeline\n",
    "    with open(pp_data_file_input, \"rb\") as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # instantiate model \n",
    "    model = MNIST()\n",
    "\n",
    "    # train model \n",
    "    model.fit_hp_search(xy_train=data[0],\n",
    "                        xy_test=data[1],\n",
    "                        )\n",
    "    \n",
    "    # define experiment name and run name\n",
    "    experiment_name = \"mnist-classification\"\n",
    "    experiment_id = mlflow_experiment_definition(experiment_name)\n",
    "    mlflow_run_name = \"deployment_run\"\n",
    "    \n",
    "    # loaded hyperparameters\n",
    "    hyperparams = {\n",
    "    'learning_rate': 0.01,\n",
    "    'l1': 0.0,\n",
    "    'l2': 0.0, \n",
    "    'num_hidden': 16,\n",
    "    'epochs': 10}\n",
    "    \n",
    "    with mlflow.start_run(experiment_id=experiment_id, \n",
    "                      run_name=mlflow_run_name) as run:\n",
    "        # You can set autolog for tensorflow model.\n",
    "        # Note that autolog does not allow logging of any additional params and metrics.\n",
    "        # We'll choose to do manual logging.\n",
    "        # mlflow.tensorflow.autolog()\n",
    "\n",
    "        model.fit(ds_train, ds_test, hyperparams)\n",
    "\n",
    "        # MLFlow Tracking parameters\n",
    "        mlflow.log_params(params=hyperparams)\n",
    "\n",
    "        # MLFlow Tracking metrics \n",
    "        # Logging metrics for each epoch (housed in dictionary)\n",
    "        training_history = model._train_history.history\n",
    "        for epoch in range(0, hyperparams['epochs']):\n",
    "            insert = {}\n",
    "            for metric, value in training_history.items():\n",
    "                insert[metric] = training_history[metric][epoch]\n",
    "            mlflow.log_metrics(metrics=insert, step=epoch+1)\n",
    "\n",
    "        # MLFlow tracking artifact (e.g. model file)\n",
    "        # this will log the model and all its details under run_id/artifacts\n",
    "        mlflow.pyfunc.log_model(python_model=model,\n",
    "                               artifact_path=\"\")\n",
    "        \n",
    "        mlflow.get_artifact_uri()\n",
    "        # Close out MLFlow run to prevent any log contamination.\n",
    "        mlflow.end_run(status='FINISHED')\n",
    "        \n",
    "    return f\"{mlflow.get_artifact_uri()}/{model.artifact_path}\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    train_op = kfp.components.create_component_from_func(\n",
    "        func=train,\n",
    "        base_image='python:3.9',\n",
    "        packages_to_install=['tensorflow', 'numpy', 'mlflow'],\n",
    "        output_component_file='train_tensorflow_mnist.yaml')\n",
    "    print('Completed transfer learning training on MNIST')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5359ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_op = kfp.components.load_component_from_file('./components/load_step/load_component.yaml')\n",
    "pp_op = kfp.components.load_component_from_file('./components/preprocess_step/preprocess_component.yaml')\n",
    "\n",
    "\n",
    "load_op(dataset_str='mnist',\n",
    "        train_test_split=True)\n",
    "\n",
    "pp_op()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74281575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:1: Invalid control characters encountered in text.\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:3: Expected identifier, got: 408422040865492794\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.getcwd()\n",
    "\n",
    "data_file_input = os.getcwd() + '/components/preprocess_step/data/'\n",
    "\n",
    "data = tf.data.Dataset.load(data_file_input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scaled-ML-deployment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
