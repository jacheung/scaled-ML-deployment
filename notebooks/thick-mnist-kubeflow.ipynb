{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23130bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and define batch sizes for tensorflow \n",
    "\n",
    "model = Model()\n",
    "ds_train = ds_train.map(preprocess_mnist_tfds, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_test = ds_test.map(preprocess_mnist_tfds, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(128) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac9474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.components import InputPath, OutputPath\n",
    "import kfp\n",
    "\n",
    "def load_tensorflow_dataset(dataset_str: str, train_file_output: OutputPath(\"pickle\"),  test_file_output: OutputPath(\"pickle\")):\n",
    "    import tensorflow_datasets as tfds\n",
    "    import pickle\n",
    "    (xy_train, xy_test), ds_info = tfds.load(\n",
    "        dataset_str,\n",
    "        split=['train', 'test'], shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True,\n",
    "    )\n",
    "    with open(train_file_output, \"wb\") as file:\n",
    "        pickle.dump(xy_train, file)\n",
    "    with open(test_file_output, \"wb\") as file:\n",
    "        pickle.dump(xy_test, file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    preprocess_op = kfp.components.create_component_from_func(\n",
    "        func=load_tensorflow_dataset,\n",
    "        base_image='python:3.9',\n",
    "        packages_to_install=['tensorflow_datasets'],\n",
    "        output_component_file='load_tensorflow_dataset.yaml')\n",
    "    print('Loaded dataset from tensorflow datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ced633b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kfp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputPath, OutputPath\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(train_file_input: InputPath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpickle\u001b[39m\u001b[38;5;124m'\u001b[39m), test_file_input: InputPath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpickle\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      5\u001b[0m               train_file_output: OutputPath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickle\u001b[39m\u001b[38;5;124m\"\u001b[39m),  test_file_output: OutputPath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kfp'"
     ]
    }
   ],
   "source": [
    "from kfp.components import InputPath, OutputPath\n",
    "import kfp\n",
    "\n",
    "def preprocess(train_file_input: InputPath('pickle'), test_file_input: InputPath('pickle'),\n",
    "              train_file_output: OutputPath(\"pickle\"),  test_file_output: OutputPath(\"pickle\")):\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "    def preprocess_mnist(image, label=None):\n",
    "        # reshape and upsample to 3 channel for transfer learning models\n",
    "        # ... for when no channel information is present\n",
    "        if len(image.shape) != 3:\n",
    "            image = np.dstack((image, image, image))\n",
    "        # ... for when channel is only 1 dimension\n",
    "        if image.shape[2] == 1:\n",
    "            image = tf.image.grayscale_to_rgb(image)\n",
    "        # normalize pixel values\n",
    "        image = tf.cast(image, tf.float32) / 255.\n",
    "        # resize with pad for mobilenetv2\n",
    "        image = tf.image.resize_with_pad(image, target_height=224, target_width=224)\n",
    "        return image, label\n",
    "    with open(train_file_input, \"rb\") as file:\n",
    "        ds_train = pickle.load(file)\n",
    "    with open(test_file_input, \"rb\") as file:\n",
    "        ds_test = pickle.load(file)\n",
    "    # preprocess and batch \n",
    "    ds_train = ds_train.map(preprocess_mnist_tfds, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds_train = ds_train.batch(128)\n",
    "    ds_test = ds_test.map(preprocess_mnist_tfds, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds_test = ds_test.batch(128) \n",
    "    \n",
    "    with open(train_file_output, \"wb\") as file:\n",
    "        pickle.dump(ds_train, file)\n",
    "    with open(test_file_output, \"wb\") as file:\n",
    "        pickle.dump(ds_test, file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    preprocess_op = kfp.components.create_component_from_func(\n",
    "        func=preprocess,\n",
    "        base_image='python:3.9',\n",
    "        packages_to_install=['numpy', 'tensorflow'],\n",
    "        output_component_file='preprocess.yaml')\n",
    "    print('Preprocessed dataset from tensorflow datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c58aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"mnist-classification\"\n",
    "mlflow_run_name = \"test_run\"\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.01,\n",
    "    'l1': 0.0,\n",
    "    'l2': 0.0, \n",
    "    'num_hidden': 16,\n",
    "    'epochs': 10}\n",
    "\n",
    "def hyperparameter_train(train_file_input: InputPath('pickle'), test_file_input: InputPath('pickle'), hyperparameters):\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "    import mlflow\n",
    "    \n",
    "    def mlflow_experiment_definition(experiment_name):\n",
    "        try:\n",
    "            experiment_id = mlflow.create_experiment(\n",
    "                experiment_name,\n",
    "                tags={\"version\": \"v0.1\"},\n",
    "            )\n",
    "        except mlflow.exceptions.MlflowException as e: \n",
    "            if str(e) == f\"Experiment '{experiment_name}' already exists.\":\n",
    "                print(f'Experiment already exists, setting experiment to {experiment_name}')\n",
    "                experiment_info = mlflow.set_experiment(\"mnist-classification\")\n",
    "                experiment_id = experiment_info.experiment_id\n",
    "        return experiment_id\n",
    "    \n",
    "    def preprocess_mnist(image, label=None):\n",
    "        # reshape and upsample to 3 channel for transfer learning models\n",
    "        # ... for when no channel information is present\n",
    "        if len(image.shape) != 3:\n",
    "            image = np.dstack((image, image, image))\n",
    "        # ... for when channel is only 1 dimension\n",
    "        if image.shape[2] == 1:\n",
    "            image = tf.image.grayscale_to_rgb(image)\n",
    "        # normalize pixel values\n",
    "        image = tf.cast(image, tf.float32) / 255.\n",
    "        # resize with pad for mobilenetv2\n",
    "        image = tf.image.resize_with_pad(image, target_height=224, target_width=224)\n",
    "        return image, label\n",
    "    \n",
    "    class MNIST(mlflow.pyfunc.PythonModel):     \n",
    "        def fit(self, xy_tuple_train, xy_tuple_test, hyperparameters):\n",
    "            ## Build model\n",
    "            # class names for mnist hardcoded\n",
    "            class_names = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "            # set layer regularization for DNN\n",
    "            regularizer = tf.keras.regularizers.l1_l2(hyperparameters['l1'], hyperparameters['l2'])\n",
    "\n",
    "            # load in mobilenetv2 weights and instantiate dense classification head \n",
    "            base_model = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "            layers = [\n",
    "                hub.KerasLayer(\n",
    "                    base_model,\n",
    "                    input_shape=(224, 224, 3),\n",
    "                    trainable=False,\n",
    "                    name='mobilenet_embedding'),\n",
    "                tf.keras.layers.Dense(hyperparameters['num_hidden'],\n",
    "                                      kernel_regularizer=regularizer,\n",
    "                                      activation='relu',\n",
    "                                      name='dense_hidden'),\n",
    "                tf.keras.layers.Dense(len(class_names),\n",
    "                                      kernel_regularizer=regularizer,\n",
    "                                      activation='softmax',\n",
    "                                      name='mnist_prob')\n",
    "            ]\n",
    "\n",
    "            self._model = tf.keras.Sequential(layers, name='mnist-classification')\n",
    "\n",
    "            # compile model \n",
    "            self._model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hyperparams['learning_rate']),\n",
    "                                loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                                from_logits=False),\n",
    "                                metrics=['accuracy'])\n",
    "\n",
    "            ## Fit model\n",
    "            # fit model and save history to model store\n",
    "            self._train_history = self._model.fit(xy_tuple_train, epochs=hyperparameters['epochs'], validation_data=xy_tuple_test)\n",
    "            self._model_base = base_model\n",
    "\n",
    "        def predict(self, context, model_input: np.ndarray) -> np.ndarray:\n",
    "            image, _ = preprocess_mnist_tfds(model_input)\n",
    "            image = tf.reshape(image, [1, 224, 224, 3])\n",
    "            return self._model.predict(image).argmax()\n",
    "    \n",
    "    # load data from pipeline\n",
    "    with open(train_file_input, \"rb\") as file:\n",
    "        ds_train = pickle.load(file)\n",
    "    with open(test_file_input, \"rb\") as file:\n",
    "        ds_test = pickle.load(file)\n",
    "    \n",
    "    # instantiate model \n",
    "    model = MNIST()\n",
    "    \n",
    "    # define experiment name and run name\n",
    "    experiment_name = \"mnist-classification\"\n",
    "    experiment_id = mlflow_experiment_definition(experiment_name)\n",
    "    mlflow_run_name = \"test_run\"\n",
    "    \n",
    "    with mlflow.start_run(experiment_id=experiment_id, \n",
    "                      run_name=mlflow_run_name) as run:\n",
    "        # You can set autolog for tensorflow model.\n",
    "        # Note that autolog does not allow logging of any additional params and metrics.\n",
    "        # We'll choose to do manual logging.\n",
    "        # mlflow.tensorflow.autolog()\n",
    "\n",
    "        model.fit(ds_train, ds_test, hyperparams)\n",
    "\n",
    "        # MLFlow Tracking parameters\n",
    "        mlflow.log_params(params=hyperparams)\n",
    "\n",
    "        # MLFlow Tracking metrics \n",
    "        # Logging metrics for each epoch (housed in dictionary)\n",
    "        training_history = model._train_history.history\n",
    "        for epoch in range(0, hyperparams['epochs']):\n",
    "            insert = {}\n",
    "            for metric, value in training_history.items():\n",
    "                insert[metric] = training_history[metric][epoch]\n",
    "            mlflow.log_metrics(metrics=insert, step=epoch+1)\n",
    "\n",
    "        # MLFlow tracking artifact (e.g. model file)\n",
    "        # this will log the model and all its details under run_id/artifacts\n",
    "        mlflow.pyfunc.log_model(python_model=model,\n",
    "                               artifact_path=\"\")\n",
    "        \n",
    "        mlflow.get_artifact_uri()\n",
    "        # Close out MLFlow run to prevent any log contamination.\n",
    "        mlflow.end_run(status='FINISHED')\n",
    "        \n",
    "    return f\"{mlflow.get_artifact_uri()}/{model.artifact_path}\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    train_op = kfp.components.create_component_from_func(\n",
    "        func=train,\n",
    "        base_image='python:3.9',\n",
    "        packages_to_install=['tensorflow', 'numpy', 'mlflow'],\n",
    "        output_component_file='train_tensorflow_mnist.yaml')\n",
    "    print('Completed transfer learning training on MNIST')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"mnist-classification\"\n",
    "mlflow_run_name = \"test_run\"\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.01,\n",
    "    'l1': 0.0,\n",
    "    'l2': 0.0, \n",
    "    'num_hidden': 16,\n",
    "    'epochs': 10}\n",
    "\n",
    "def production_train(train_file_input: InputPath('pickle'), test_file_input: InputPath('pickle'), hyperparameters):\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "    import mlflow\n",
    "    \n",
    "    def mlflow_experiment_definition(experiment_name):\n",
    "        try:\n",
    "            experiment_id = mlflow.create_experiment(\n",
    "                experiment_name,\n",
    "                tags={\"version\": \"v0.1\"},\n",
    "            )\n",
    "        except mlflow.exceptions.MlflowException as e: \n",
    "            if str(e) == f\"Experiment '{experiment_name}' already exists.\":\n",
    "                print(f'Experiment already exists, setting experiment to {experiment_name}')\n",
    "                experiment_info = mlflow.set_experiment(\"mnist-classification\")\n",
    "                experiment_id = experiment_info.experiment_id\n",
    "        return experiment_id\n",
    "    \n",
    "    def preprocess_mnist(image, label=None):\n",
    "        # reshape and upsample to 3 channel for transfer learning models\n",
    "        # ... for when no channel information is present\n",
    "        if len(image.shape) != 3:\n",
    "            image = np.dstack((image, image, image))\n",
    "        # ... for when channel is only 1 dimension\n",
    "        if image.shape[2] == 1:\n",
    "            image = tf.image.grayscale_to_rgb(image)\n",
    "        # normalize pixel values\n",
    "        image = tf.cast(image, tf.float32) / 255.\n",
    "        # resize with pad for mobilenetv2\n",
    "        image = tf.image.resize_with_pad(image, target_height=224, target_width=224)\n",
    "        return image, label\n",
    "    \n",
    "    class MNIST(mlflow.pyfunc.PythonModel):     \n",
    "        def fit(self, xy_tuple_train, xy_tuple_test, hyperparameters):\n",
    "            ## Build model\n",
    "            # class names for mnist hardcoded\n",
    "            class_names = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "            # set layer regularization for DNN\n",
    "            regularizer = tf.keras.regularizers.l1_l2(hyperparameters['l1'], hyperparameters['l2'])\n",
    "\n",
    "            # load in mobilenetv2 weights and instantiate dense classification head \n",
    "            base_model = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "            layers = [\n",
    "                hub.KerasLayer(\n",
    "                    base_model,\n",
    "                    input_shape=(224, 224, 3),\n",
    "                    trainable=False,\n",
    "                    name='mobilenet_embedding'),\n",
    "                tf.keras.layers.Dense(hyperparameters['num_hidden'],\n",
    "                                      kernel_regularizer=regularizer,\n",
    "                                      activation='relu',\n",
    "                                      name='dense_hidden'),\n",
    "                tf.keras.layers.Dense(len(class_names),\n",
    "                                      kernel_regularizer=regularizer,\n",
    "                                      activation='softmax',\n",
    "                                      name='mnist_prob')\n",
    "            ]\n",
    "\n",
    "            self._model = tf.keras.Sequential(layers, name='mnist-classification')\n",
    "\n",
    "            # compile model \n",
    "            self._model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hyperparams['learning_rate']),\n",
    "                                loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                                from_logits=False),\n",
    "                                metrics=['accuracy'])\n",
    "\n",
    "            ## Fit model\n",
    "            # fit model and save history to model store\n",
    "            self._train_history = self._model.fit(xy_tuple_train, epochs=hyperparameters['epochs'], validation_data=xy_tuple_test)\n",
    "            self._model_base = base_model\n",
    "\n",
    "        def predict(self, context, model_input: np.ndarray) -> np.ndarray:\n",
    "            image, _ = preprocess_mnist_tfds(model_input)\n",
    "            image = tf.reshape(image, [1, 224, 224, 3])\n",
    "            return self._model.predict(image).argmax()\n",
    "    \n",
    "    # load data from pipeline\n",
    "    with open(train_file_input, \"rb\") as file:\n",
    "        ds_train = pickle.load(file)\n",
    "    with open(test_file_input, \"rb\") as file:\n",
    "        ds_test = pickle.load(file)\n",
    "    \n",
    "    # instantiate model \n",
    "    model = MNIST()\n",
    "    \n",
    "    # define experiment name and run name\n",
    "    experiment_name = \"mnist-classification\"\n",
    "    experiment_id = mlflow_experiment_definition(experiment_name)\n",
    "    mlflow_run_name = \"deployment_run\"\n",
    "    \n",
    "    # loaded hyperparameters\n",
    "    hyperparams = {\n",
    "    'learning_rate': 0.01,\n",
    "    'l1': 0.0,\n",
    "    'l2': 0.0, \n",
    "    'num_hidden': 16,\n",
    "    'epochs': 10}\n",
    "    \n",
    "    with mlflow.start_run(experiment_id=experiment_id, \n",
    "                      run_name=mlflow_run_name) as run:\n",
    "        # You can set autolog for tensorflow model.\n",
    "        # Note that autolog does not allow logging of any additional params and metrics.\n",
    "        # We'll choose to do manual logging.\n",
    "        # mlflow.tensorflow.autolog()\n",
    "\n",
    "        model.fit(ds_train, ds_test, hyperparams)\n",
    "\n",
    "        # MLFlow Tracking parameters\n",
    "        mlflow.log_params(params=hyperparams)\n",
    "\n",
    "        # MLFlow Tracking metrics \n",
    "        # Logging metrics for each epoch (housed in dictionary)\n",
    "        training_history = model._train_history.history\n",
    "        for epoch in range(0, hyperparams['epochs']):\n",
    "            insert = {}\n",
    "            for metric, value in training_history.items():\n",
    "                insert[metric] = training_history[metric][epoch]\n",
    "            mlflow.log_metrics(metrics=insert, step=epoch+1)\n",
    "\n",
    "        # MLFlow tracking artifact (e.g. model file)\n",
    "        # this will log the model and all its details under run_id/artifacts\n",
    "        mlflow.pyfunc.log_model(python_model=model,\n",
    "                               artifact_path=\"\")\n",
    "        \n",
    "        mlflow.get_artifact_uri()\n",
    "        # Close out MLFlow run to prevent any log contamination.\n",
    "        mlflow.end_run(status='FINISHED')\n",
    "        \n",
    "    return f\"{mlflow.get_artifact_uri()}/{model.artifact_path}\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    train_op = kfp.components.create_component_from_func(\n",
    "        func=train,\n",
    "        base_image='python:3.9',\n",
    "        packages_to_install=['tensorflow', 'numpy', 'mlflow'],\n",
    "        output_component_file='train_tensorflow_mnist.yaml')\n",
    "    print('Completed transfer learning training on MNIST')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8855fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.integration.mlflow import MLflowCallback\n",
    "\n",
    "\n",
    "def preprocess_mnist(image, label=None):\n",
    "    # reshape and upsample to 3 channel for transfer learning models\n",
    "    # ... for when no channel information is present\n",
    "    if len(image.shape) != 3:\n",
    "        image = np.dstack((image, image, image))\n",
    "    # ... for when channel is only 1 dimension\n",
    "    if image.shape[2] == 1:\n",
    "        image = tf.image.grayscale_to_rgb(image)\n",
    "    # normalize pixel values\n",
    "    image = tf.cast(image, tf.float32) / 255.\n",
    "    # resize with pad for mobilenetv2\n",
    "    image = tf.image.resize_with_pad(image, target_height=224, target_width=224)\n",
    "    return image, label\n",
    "\n",
    "class MNIST(mlflow.pyfunc.PythonModel):     \n",
    "    def fit(self, xy_tuple_train, xy_tuple_test, hyperparameters):\n",
    "        ## Build model\n",
    "        # class names for mnist hardcoded\n",
    "        class_names = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "        # set layer regularization for DNN\n",
    "        regularizer = tf.keras.regularizers.l1_l2(hyperparameters['l1'], hyperparameters['l2'])\n",
    "\n",
    "        # load in mobilenetv2 weights and instantiate dense classification head \n",
    "        base_model = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "        layers = [\n",
    "            hub.KerasLayer(\n",
    "                base_model,\n",
    "                input_shape=(224, 224, 3),\n",
    "                trainable=False,\n",
    "                name='mobilenet_embedding'),\n",
    "            tf.keras.layers.Dense(hyperparameters['num_hidden'],\n",
    "                                  kernel_regularizer=regularizer,\n",
    "                                  activation='relu',\n",
    "                                  name='dense_hidden'),\n",
    "            tf.keras.layers.Dense(len(class_names),\n",
    "                                  kernel_regularizer=regularizer,\n",
    "                                  activation='softmax',\n",
    "                                  name='mnist_prob')\n",
    "        ]\n",
    "\n",
    "        self._model = tf.keras.Sequential(layers, name='mnist-classification')\n",
    "\n",
    "        # compile model \n",
    "        self._model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hyperparams['learning_rate']),\n",
    "                            loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                            from_logits=False),\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "        ## Fit model\n",
    "        # fit model and save history to model store\n",
    "        self._train_history = self._model.fit(xy_tuple_train, epochs=hyperparameters['epochs'], validation_data=xy_tuple_test)\n",
    "        self._model_base = base_model\n",
    "\n",
    "    def predict(self, context, model_input: np.ndarray) -> np.ndarray:\n",
    "        image, _ = preprocess_mnist_tfds(model_input)\n",
    "        image = tf.reshape(image, [1, 224, 224, 3])\n",
    "        return self._model.predict(image).argmax()\n",
    "\n",
    "# hyperparameters search using Optuna\n",
    "# can scale Optuna with Kubeflow https://medium.com/optuna/parallel-hyperparameter-tuning-with-optuna-and-kubeflow-pipelines-4ef05ce614ae\n",
    "def objective(trial): \n",
    "    \"\"\"\n",
    "    Optuna objective function for tuning transfer learning model\n",
    "    \"\"\"\n",
    "    hyperparameters = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.00001, 0.1, log=True),\n",
    "        'l1': trial.suggest_float('l1', 0.0, 0.1),\n",
    "        'l2': trial.suggest_float('l2', 0.0, 0.1),\n",
    "        'num_hidden': trial.suggest_int('num_hidden', 8, 64),\n",
    "        'epochs': trial.suggest_int('epochs', 2, 10)\n",
    "    }\n",
    "\n",
    "    model.fit(ds_train, ds_test, hyperparameters)\n",
    "    training_history = model._train_history.history\n",
    "    validation_accuracy = training_history['val_accuracy'][-1]\n",
    "    return validation_accuracy\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print('loading study...')\n",
    "        study = optuna.load_study(\n",
    "            study_name=\"mnist-classification-kubernetes\",\n",
    "            storage=\"postgresql://{}:{}@postgres:5432/{}\".format(\n",
    "                os.environ[\"POSTGRES_USER\"],\n",
    "                os.environ[\"POSTGRES_PASSWORD\"],\n",
    "                os.environ[\"POSTGRES_DB\"],\n",
    "            ),\n",
    "        )\n",
    "    except KeyError:\n",
    "        print('no study found. building from scratch...')\n",
    "        study = optuna.create_study(\n",
    "            study_name=\"mnist-classification-kubernetes\",\n",
    "            storage=\"postgresql://{}:{}@postgres:5432/{}\".format(\n",
    "                os.environ[\"POSTGRES_USER\"],\n",
    "                os.environ[\"POSTGRES_PASSWORD\"],\n",
    "                os.environ[\"POSTGRES_DB\"],\n",
    "            ),\n",
    "            pruner=optuna.pruners.HyperbandPruner(),\n",
    "            direction='maximize'\n",
    "        )\n",
    "\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    print(study.best_trial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162c616",
   "metadata": {},
   "source": [
    "### yaml file for Kubernetes of Optuna job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3888020c",
   "metadata": {},
   "source": [
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: study-creator\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      restartPolicy: OnFailure\n",
    "      initContainers:\n",
    "        - name: wait-for-database\n",
    "          image: postgres:latest\n",
    "          imagePullPolicy: IfNotPresent\n",
    "          command:\n",
    "          - /bin/sh\n",
    "          - -c\n",
    "          - -e\n",
    "          - -x\n",
    "          - |\n",
    "            until pg_isready -U $(POSTGRES_USER) -h postgres -p 5432;\n",
    "            do echo \"waiting for postgres\"; sleep 2; done;\n",
    "          envFrom:\n",
    "            - secretRef:\n",
    "                name: postgres-secrets\n",
    "      containers:\n",
    "        - name: study-creator\n",
    "          image: optuna-kubernetes:example\n",
    "          imagePullPolicy: IfNotPresent\n",
    "          command:\n",
    "          # create study\n",
    "          - /bin/sh\n",
    "          - -c\n",
    "          - -e\n",
    "          - -x\n",
    "          - |\n",
    "            optuna create-study --skip-if-exists --direction maximize \\\n",
    "            --study-name \"kubernetes\" --storage \\\n",
    "            \"postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}\"\n",
    "          envFrom:\n",
    "            - secretRef:\n",
    "                name: postgres-secrets\n",
    "---\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: worker\n",
    "spec:\n",
    "  parallelism: 5\n",
    "  template:\n",
    "    spec:\n",
    "      restartPolicy: OnFailure\n",
    "      initContainers:\n",
    "        - name: wait-for-study\n",
    "          image: optuna-kubernetes:example\n",
    "          imagePullPolicy: IfNotPresent\n",
    "          command:\n",
    "          - /bin/sh\n",
    "          - -c\n",
    "          - -e\n",
    "          - -x\n",
    "          - |\n",
    "            until [ `sh check_study.sh` -eq 0 ];\n",
    "            do echo \"waiting for study\"; sleep 2; done;\n",
    "          envFrom:\n",
    "            - secretRef:\n",
    "                name: postgres-secrets\n",
    "      containers:\n",
    "        - name: worker\n",
    "          image: optuna-kubernetes:example\n",
    "          imagePullPolicy: IfNotPresent\n",
    "          command:\n",
    "            - python\n",
    "            - sklearn_distributed.py\n",
    "          envFrom:\n",
    "            - secretRef:\n",
    "                name: postgres-secrets\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thick-ML-deployment",
   "language": "python",
   "name": "thick-ml-deployment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
